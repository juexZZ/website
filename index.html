<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Juexiao Zhang</title>
  
  <meta name="author" content="Juexiao Zhang adopted from Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üêØ</text></svg>">
  <link rel="icon" href="images/tiger-emoji.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Juexiao Zhang</name>
              </p>
              <p>
                Hi, I am currently a second year PhD student in computer science at the <a href="https://cs.nyu.edu/home/index.html">Courant Institute at New York University</a> advised by Professor <a href="https://engineering.nyu.edu/faculty/chen-feng">Chen Feng</a>. 
                My First name in Chinese is ËßâÊôì. It comes from a famous <a href="https://frommetertomeaning.wordpress.com/a-spring-morning-meng-haoran-Êò•Êôì-Â≠üÊµ©ÁÑ∂/">poem</a> in Tang Dynasty (618-907 AD), which depicts a quiet spring morning. 
                I also go by my English nickname Jeremy.
              </p>
              <p>
                Previously, I obtained my Bachelor's degree in EE from Tsinghua University and my Master's degree in CS from NYU. 
                During my Master's I was fortunate to work with Prof. Chen Feng AI4CE Lab on scene representation for robotics and Dr. Yubei Chen on unsupervised representation learning. 
                I am interested in learning scene representations that are useful for robots to understand the world and interact with it.
              </p>
              <p style="text-align:center">
                <a href="mailto:jz4725@nyu.edu">Email</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/juexiao-zhang-788453146/">LinkedIn</a> &nbsp/&nbsp
                <!-- <a href="data/JuexiaoZhang-CV.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="data/JuexiaoZhang-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=TYxPbcEAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/juexZZ">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Juexiao.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Juexiao-1.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Education</heading>
            <p>
              <a href="https://cs.nyu.edu/home/index.html">New York University</a>, 
              Ph.D in Computer Science, 2023 - present
              <br>
              <a href="https://cs.nyu.edu/home/index.html">New York University</a>, 
              M.S. in Computer Science, 2021 - 2023
              <br>
              <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>,
              B.S. in Electrical Engineering, 2016 - 2020
            </p>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:16px;width:100%;vertical-align:middle">
          <heading>Research</heading>
          <p>
            <!-- fill in: research interest or others -->
          </p>
        </td>
      </tr>
    </tbody></table>
      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/citywalker.png' width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2310.04496">
              <papertitle>CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos</papertitle>
            </a>
            <br>
            <a href="https://gaaaavin.github.io/">Xinhao Liu</a>,
            <a href="">Jintong Li</a>, 
            <a href="">Yicheng Jiang</a>,
            <a href="">Niranjan Sujay</a>,  
            <a href="">Zhicheng Yang</a>, 
            <strong>Juexiao Zhang</strong>,
            <a href="">Jone Abanes</a>,
            <a href="https://www.linkedin.com/in/jing-zhang-4b7163285/">Jing Zhang</a>,   
            <a href="https://scholar.google.com/citations?user=YeG8ZM0AAAAJ">Chen Feng</a>, 
            <br>
            <em>Under review.</em>
            <br>
            <a href="https://arxiv.org/abs/2411.17820">arXiv</a>
              <!-- / -->
            <a href="https://ai4ce.github.io/CityWalker/">Project Page</a>
            /
            <a href="https://github.com/ai4ce/CityWalker">Code</a>
            /
            <a href="https://huggingface.co/datasets/ai4ce/CityWalker">Data</a>
            <p></p>
            <p>We leverage thousands of hours of online city walking and driving videos to train autonomous agents for robust, generalizable navigation in urban environments through scalable, data-driven imitation learning.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/urlost.jpg' width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2310.04496">
              <papertitle>URLOST: Unsupervised Representation Learning without Stationarity or Topology</papertitle>
            </a>
            <br>
            <a href="https://scholar.google.com/citations?user=6WadrlgAAAAJ&hl=en">Zeyu Yun</a>, 
            <strong>Juexiao Zhang</strong>, 
            <a href="https://yann.lecun.com/">Yann LeCun</a>, 
            <a href="https://yubeichen.com/">Yubei Chen</a>
            <br>
            <em>ICLR 2025</em>
            <br>
            <a href="https://arxiv.org/abs/2310.04496">arXiv</a>
              <!-- / -->
            <!-- <a href="https://coperception.github.io/star/">Project Page</a> -->
            <p></p>
            <p>An unsupervised representation learning for high-dimensional data without explicit stationarity and topology.</p>
          </td>
        </tr>

        <tr onmouseout="msg_stop()" onmouseover="msg_start()">
          <!-- <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/msg-nips24/msg.jpg' width="200">
          </td> -->
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='msg_after'>
                <video width=100% height=100% id="msg_video" muted autoplay loop>
                  <source src="./images/msg-nips24/msg_after.mp4" type="video/mp4">
                </video>
              </div>

              <img src='./images/msg-nips24/msg.jpg' width="160" height="160" id="msg_image">
            </div>
            <script type="text/javascript">
              
              function msg_start() {
                // when mouse is over the image, make the video visible
                document.getElementById('msg_after').style.opacity = "1";
                // make the image invisible
                document.getElementById('msg_image').style.opacity = "0";
                document.getElementById('msg_video').play(); // somehow this can prevent jittering
              }

              function msg_stop() {
                document.getElementById('msg_after').style.opacity = "0";
                //make the image visible again
                document.getElementById('msg_image').style.opacity = "1";
                document.getElementById('msg_video').pause(); // somehow this can prevent jittering
                document.getElementById('msg_video').currentTime = 0;
              }
              msg_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://ai4ce.github.io/MSG/">
              <papertitle>Multiview Scene Graph</papertitle>
            </a>
            <br>
            <strong>Juexiao Zhang</strong>, 
            <a href="">Gao Zhu</a>,
            <a href="https://louis-leee.github.io/">Sihang Li</a>, 
            <a href="https://gaaaavin.github.io/">Xinhao Liu</a>, 
            <a href="">Haorui Song</a>,
            <a href="">Xinran Tang</a>,
            <a href="https://scholar.google.com/citations?user=YeG8ZM0AAAAJ&hl=en">Chen Feng</a>
            <br>
            <em>NeurIPS 2024</em>
            <br>
            <a href="https://arxiv.org/abs/2410.11187">arXiv</a>
            /
            <a href="https://ai4ce.github.io/MSG/">project page</a>
            /
            <a href="https://github.com/ai4ce/MSG">code</a>
            <!-- <em> Camera ready coming soon. </em> -->
            <p></p>
            <p>Build multiview place+object scene graph from unposed RGB image set.</p>
          </td>
        </tr>

        <tr onmouseout="seedo_stop()" onmouseover="seedo_start()">
          <!-- <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/seedo.jpg' width="200">
          </td> -->
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='seedo_after'>
                <video width=100% height=100% id="seedo_video" muted autoplay loop>
                  <source src="./images/seedo/seedo_after.mp4" type="video/mp4">
                </video>
              </div>

              <img src='./images/seedo/seedo.jpg' width="160" height="160" id="seedo_image">
            </div>
            <script type="text/javascript">
              
              function seedo_start() {
                // when mouse is over the image, make the video visible
                document.getElementById('seedo_after').style.opacity = "1";
                // make the image invisible
                document.getElementById('seedo_image').style.opacity = "0";
                document.getElementById('seedo_video').play(); // somehow this can prevent jittering
              }

              function seedo_stop() {
                document.getElementById('seedo_after').style.opacity = "0";
                //make the image visible again
                document.getElementById('seedo_image').style.opacity = "1";
                document.getElementById('seedo_video').pause(); // somehow this can prevent jittering
                document.getElementById('seedo_video').currentTime = 0;
              }
              seedo_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://ai4ce.github.io/SeeDo/">
              <papertitle>VLM See, Robot Do:
                Human Demo Video to Robot Action Plan via Vision Language Model</papertitle>
            </a>
            <br>
            <a href="https://scholar.google.com/citations?user=ACFrFnYAAAAJ&hl=en">Beichen Wang*</a>, 
            <strong>Juexiao Zhang*</strong>, 
            <a href="https://www.linkedin.com/in/shuwen-dong-1342832b1/">Shuwen Dong&dagger;</a>, 
            <a href="https://irvingf7.github.io/">Irving Fang&dagger;</a>, 
            <a href="https://scholar.google.com/citations?user=YeG8ZM0AAAAJ&hl=en">Chen Feng</a>
            (*&dagger; for equal contribution)
            <br>
            <em>Under review.</em>
            <br>
            <a href="">arXiv</a>
            /
            <a href="https://ai4ce.github.io/SeeDo/">project page</a>
            /
            <a href="https://github.com/ai4ce/SeeDo">code</a>
            <p></p>
            <p>Let the robot follow a human's actions by just watching one video.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/LLM4VPR.jpg' width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://ai4ce.github.io/LLM4VPR/">
              <papertitle>Tell Me Where You Are: Multimodal LLMs Meet Place Recognition</papertitle>
            </a>
            <br>
            <a href="https://zonglinl.github.io/">Zonglin Lyu</a>, <strong>Juexiao Zhang</strong>, <a href="https://scholar.google.com/citations?user=m4ChlREAAAAJ&hl=en">Mingxuan Lu</a>, <a href="https://yimingli-page.github.io/">Yiming Li</a>, <a href="https://scholar.google.com/citations?user=YeG8ZM0AAAAJ&hl=en">Chen Feng</a>
            <br>
            <em>Under review.</em>
            <br>
            <a href="https://arxiv.org/abs/2406.17520">arXiv</a>
            /
            <a href="https://ai4ce.github.io/LLM4VPR/">project page</a>
            /
            <a href="https://github.com/ai4ce/LLM4VPR">code</a>
            <!-- <em> Code coming soon. </em> -->
            <p></p>
            <p>Can multimodal LLMs help visual place recognition?</p>
          </td>
        </tr>

        <tr onmouseout="luwa_stop()" onmouseover="luwa_start()" >
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='luwa_image'>
                <img src='images/luwa-cvpr24/luwa2.jpg' width="160"></div>
              <img src='images/luwa-cvpr24/luwa1.jpg' width="160">
            </div>
            <script type="text/javascript">
              function luwa_start() {
                document.getElementById('luwa_image').style.opacity = "1";
              }

              function luwa_stop() {
                document.getElementById('luwa_image').style.opacity = "0";
              }
              luwa_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://ai4ce.github.io/LUWA/">
              <papertitle>LUWA Dataset: Learning Lithic Use-Wear Analysis on Microscopic Images</papertitle>
            </a>
            <br>
            <a href="https://www.linkedin.com/in/jing-zhang-4b7163285/"> Jing Zhang*</a>,
            <a href="https://irvingf7.github.io/">Irving Fang*</a>, 
            <a href="">Hao Wu</a>, 
            <a href="https://www.linkedin.com/in/akshat-kaushik/">Akshat Kaushik</a>, 
            <a href="https://as.nyu.edu/departments/anthropology/people/graduate-students/doctoral-students/alice-rodriguez.html">Alice Rodriguez</a>, 
            <a href="https://www.linkedin.com/in/hanwen-zhao-2523a4104/">Hanwen Zhao</a>, 
            <strong>Juexiao Zhang</strong>, 
            <a href="">Zhuo Zheng</a>, 
            <a href="https://wp.nyu.edu/faculty-iovita/">Radu Iovita</a>,
            <a href="https://scholar.google.com/citations?user=YeG8ZM0AAAAJ">Chen Feng</a>
            (* for equal contribution)
            <br>
            <em>CVPR</em>, 2024. <strong> Highlight</strong>
            <br>
            <a href="https://ai4ce.github.io/LUWA/">project page</a>
            /
            <a href="https://arxiv.org/abs/2403.13171">arXiv</a>
            /
            <a href="https://github.com/ai4ce/LUWA">code</a>
            
            <p></p>
            <p>Paleoanthropology meets cutting-edge computer vision! <br> We create the first Lithic Use-Wear Analysis (LUWA) dataset and challenge Large Vision Model and Large Language and Vision Model with it.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/actformer.png' width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://coperception.github.io/ActFormer/">
              <papertitle>Actformer: Scalable Collaborative Perception via Active Queries</papertitle>
            </a>
            <br>
            <a href="https://hsz0403.github.io/">Suozhi Huang*</a>, <strong>Juexiao Zhang*</strong>, <a href="https://yimingli-page.github.io/">Yiming Li</a>, <a href="https://scholar.google.com/citations?user=YeG8ZM0AAAAJ&hl=en">Chen Feng</a>
            (* for equal contribution)
            <br>
            <em>ICRA, 2024</em>
            <br>
            <a href="https://arxiv.org/abs/2403.04968">arXiv</a>
            /
            <a href="https://coperception.github.io/ActFormer/">project page</a>
            /
            <a href="https://github.com/coperception/ActFormer">code</a>
            <p></p>
            <p> A collaborative BEV Transformer for 3D object detection where each BEV query can actively select relevant cameras for information aggregation based on their pose information, instead of interacting with all cameras indiscriminately.</p>
          </td>
        </tr>
					
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/corl-star.jpg' width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://openreview.net/forum?id=hW0tcXOJas2">
              <papertitle>Multi-Robot Scene Completion: Towards Task-Agnostic Collaborative Perception</papertitle>
            </a>
            <br>
            <a href="https://roboticsyimingli.github.io/">Yiming Li*</a>, <strong>Juexiao Zhang*</strong>, <a href="https://dekun.me/">Dekun Ma</a>, <a href="https://yuewang.xyz/">Yue Wang</a>, <a href="https://engineering.nyu.edu/faculty/chen-feng">Chen Feng</a>
            (* for equal contribution)
            <br>
            <em>CoRL</em>, 2022
            <br>
            <!-- <a href="data/Shelhamer2015.bib">bibtex</a> -->
            <a href="https://openreview.net/forum?id=hW0tcXOJas2">paper</a>
            /
            <a href="https://coperception.github.io/star/">project page</a>
            /
            <a href="https://github.com/coperception/star">code</a>
            <p></p>
            <p>A task-tgnostic framwork that allows asynchronous training for Collaborative perception. An autoencoder that amortizes communication in spatial-temporal domain.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/word-emb-vis.jpg' width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Word Embedding Visualization via Dictionary Learning</papertitle>
            <br>
            <strong>Juexiao Zhang*</strong>, <a href="https://yubeichen.com/">Yubei Chen*</a>, <a href="https://scholar.google.com/citations?user=7N-ethYAAAAJ&hl=en">Brian Cheung</a>, <a href="http://www.rctn.org/bruno/">Bruno Olshausen</a>
            (* for equal contribution)
            <br>
            <em>arXiv preprint arXiv:1910.03833</em>
            <br>
            <a href="https://arxiv.org/abs/1910.03833">paper</a>
            /
            <a href="https://github.com/juexZZ/WordEmbVis">code</a>
            <p></p>
            <p>Decomposed word embedding via dictionary learning and spectral clustering and discover elemantary semantic factors.</p>
          </td>
        </tr>
        <!-- template from Jon Barron -->
          <!-- <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dreamfusion_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/dreamfusion.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/dreamfusion.jpg' width="160">
              </div>
              <script type="text/javascript">
                function dreamfusion_start() {
                  document.getElementById('dreamfusion_image').style.opacity = "1";
                }

                function dreamfusion_stop() {
                  document.getElementById('dreamfusion_image').style.opacity = "0";
                }
                dreamfusion_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://dreamfusion3d.github.io/">
                <papertitle>DreamFusion: Text-to-3D using 2D Diffusion</papertitle>
              </a>
              <br>
              <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>,
              <a href="https://www.ajayj.com/">Ajay Jain</a>,
              <strong>Jonathan T. Barron</strong>,
							<a href="https://bmild.github.io/">Ben Mildenhall</a>
              <br>
              <em>arXiv</em>, 2022
              <br>
              <a href="https://dreamfusion3d.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
              /
              <a href="https://dreamfusion3d.github.io/gallery.html">gallery</a>
              <p></p>
              <p>
              We optimize a NeRF from scratch using a pretrained text-to-image diffusion model to do text-to-3D generative modeling.
              </p>
            </td>
          </tr> -->
					
        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
              <p>
                Reviewer for ICRA 2024, IROS 2024, NeurIPS 2024 (<i>Top reviewer</i>), ICLR 2025. 
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Awards/Scholarships</heading>
            <p>
              National Scholarship in Tsinghua University, 2017
            <br>
              Scholarship of Academic Excellence in Tsinghua University, 2017
            <br>
              Scholarship of Outstanding Voluntary Work in Tsinghua University, 2017
            </p>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td>
            <heading>Misc</heading>
            <p>
              In my spare time, I enjoy playing soccer, making coffee, reading, photograph and travel.</a> 
            </p>
          </td>
        </tr>
      </tbody></table>

      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:0px">
            <br>
            <p style="text-align:right;font-size:small;">
              <br>
              Website template from <a href="https://jonbarron.info/">Jon Barron</a>'s, <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
            </p>
          </td>
        </tr>
      </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
